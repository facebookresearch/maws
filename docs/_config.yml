title: The effectiveness of MAE pre-pretraining for billion-scale pretraining
description: >- # this means to ignore newlines until "baseurl:"
  Project website for the paper "The effectiveness of MAE pre-pretraining for 
  billion-scale pretraining". Contains links to an interactive Colab notebook
  with a multilingual zero shot demo, github link to the released MAE (Masked
  Autoencoder) and MAWS (MAE->WSP) models. 
baseurl: "" # the subpath of your site, e.g. /blog
url: "https://mannatsingh.github.io/maws" # the base hostname & protocol for your site, e.g. http://example.com
# twitter_username: jekyllrb
# github_username:  jekyll

# Build settings
theme: minima
plugins:
