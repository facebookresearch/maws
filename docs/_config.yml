title: The effectiveness of MAE pre-pretraining for billion-scale pretraining
description: >- # this means to ignore newlines until "baseurl:"
  Project website for the paper "The effectiveness of MAE pre-pretraining for 
  billion-scale pretraining". Contains links to an interactive Colab notebook
  with a multilingual zero shot demo, github link to the released MAE (Masked
  Autoencoder) and MAWS (MAE->WSP) models, and link to the paper on arxiv. 
baseurl: "/maws" # the subpath of your site, e.g. /blog
url: "https://facebookresearch.github.io" # the base hostname & protocol for your site, e.g. http://example.com

# Build settings
theme: minima
plugins:
