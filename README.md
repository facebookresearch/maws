# MAWS
Code and models for the paper "The effectiveness of MAE pre-pretraining for billion-scale pretraining"

# MAWS pretrained models

Model | IN1k linear | IN1k 512px finetuned | Text encoder | IN1k 0-shot 
--- | --- | --- | --- | --- 
ViT-B | 83.3 | 86.4* | XLMR-B | 74.9
ViT-L | 86.1 | 88.8* | XLMR-L | 79.7
ViT-H | 87.5 | 89.4 | XLMR-L | 81.1
ViT-2B | 88.1 | 89.7 | XLMR-L | 82.1

*tentative

# MAE pretrained models

Model | IN1k 224px finetuned
--- | --- 
ViT-B | 
ViT-L |
ViT-H | 
ViT-2B | 
ViT-2B-IN1k |